{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiModal Modeling Image To Text\n",
    "\n",
    "https://huggingface.co/docs/transformers/master/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel\n",
    "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://packages.microsoft.com/repos/azure-cli focal InRelease\n",
      "Hit:2 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease\n",
      "Hit:3 https://cli.github.com/packages focal InRelease                          \u001b[0m\n",
      "Hit:4 http://ppa.launchpad.net/xapienz/curl34/ubuntu focal InRelease           \u001b[0m\n",
      "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease               \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease     \u001b[0m                \n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \u001b[0m\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease       \u001b[0m    \n",
      "Hit:9 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease[0m\n",
      "Hit:10 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease   \u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
      "python3-venv is already the newest version (3.8.2-0ubuntu2).\n",
      "python3-pip is already the newest version (20.0.2-5ubuntu1.6).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y python3-dev python3-pip python3-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ipykernel transformers pillow requests urllib3 certifi idna pyyaml torch tensorflow ipywidgets jupyter jupyter_contrib_nbextensions --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 15:21:29.822377: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-15 15:21:29.822419: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 15:21:51.733378: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-15 15:21:51.733428: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-15 15:21:51.733461: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces_f9f5a2): /proc/driver/nvidia/version does not exist\n",
      "2022-02-15 15:21:51.734414: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFViTModel.\n",
      "\n",
      "All the layers of TFViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n",
      "2022-02-15 15:21:59.545470: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-02-15 15:21:59.650352: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-02-15 15:21:59.990896: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-02-15 15:22:00.036211: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-02-15 15:22:01.862600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-02-15 15:22:05.781770: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "Some layers of TFGPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._5/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._6/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/c_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._2/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._8/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/c_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._9/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._1/crossattention/q_attn/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._7/ln_cross_attn/gamma:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/ln_cross_attn/beta:0', 'tf_vision_encoder_decoder_model/transformer/h_._10/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._11/crossattention/c_proj/bias:0', 'tf_vision_encoder_decoder_model/transformer/h_._0/crossattention/q_attn/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._4/crossattention/c_proj/weight:0', 'tf_vision_encoder_decoder_model/transformer/h_._3/crossattention/c_proj/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized\n",
    "model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n",
    ")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# forward\n",
    "pixel_values = feature_extractor(images=img, return_tensors=\"tf\").pixel_values  # Batch size 1\n",
    "decoder_input_ids = decoder_tokenizer(\"Linda Davis\", return_tensors=\"tf\").input_ids  # Batch size 1\n",
    "outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 50257])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.4185678e-03 2.6717638e-03 2.4844165e-04 ... 5.7827606e-07\n",
      "   2.0972002e-06 1.5366605e-03]\n",
      "  [2.8044521e-04 1.1812281e-02 1.4363402e-05 ... 1.9583121e-08\n",
      "   1.0469118e-05 3.4460629e-04]\n",
      "  [1.3529514e-04 4.7747428e-03 1.2293266e-05 ... 3.2709604e-08\n",
      "   2.5662534e-06 1.2595564e-03]]], shape=(1, 3, 50257), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)\n",
    "loss, logits = outputs.loss, outputs.logits\n",
    "print(loss)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"vit-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VIT-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFVisionEncoderDecoderModel.\n",
      "\n",
      "All the layers of TFVisionEncoderDecoderModel were initialized from the model checkpoint at vit-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFVisionEncoderDecoderModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# save and load from pretrained\n",
    "model = TFVisionEncoderDecoderModel.from_pretrained(\"vit-gpt2\")\n",
    "\n",
    "# generation\n",
    "generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_of_car = \"https://d1ix0byejyn2u7.cloudfront.net/drive/images/uploads/headers/ws_cropper/1_0x0_1520x1040_0x520_car-body-types-mazda-cx5.jpg\"\n",
    "img = Image.open(requests.get(image_of_car, stream=True).raw)\n",
    "pixel_values = feature_extractor(images=img, return_tensors=\"tf\").pixel_values  # Batch size 1\n",
    "generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_tokenizer.batch_decode(generated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_of_number = \"https://www.researchgate.net/profile/Thomas-Wiatowski/publication/287853768/figure/fig5/AS:667034937462785@1536044930131/Handwritten-digits-from-the-MNIST-data-set-5-If-f-denotes-the-image-of-the-handwritten.jpg\"\n",
    "img = Image.open(requests.get(image_of_number, stream=True).raw)\n",
    "pixel_values = feature_extractor(images=img, return_tensors=\"tf\").pixel_values  # Batch size 1\n",
    "generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)\n",
    "decoder_tokenizer.batch_decode(generated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load image from the IAM dataset\n",
    "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "# training\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "text = \"hello world\"\n",
    "labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "loss = outputs.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 20:10:50.240141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-14 20:10:50.240178: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'industry, \" Mr. Brown commented icily. \" Let us have a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# inference (generation)\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisionEncoderDecoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n",
    "\n",
    "# Initializing a ViT & BERT style configuration\n",
    "config_encoder = ViTConfig()\n",
    "config_decoder = BertConfig()\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# Initializing a ViTBert model from a ViT & bert-base-uncased style configurations\n",
    "model = VisionEncoderDecoderModel(config=config)\n",
    "\n",
    "# Accessing the model configuration\n",
    "config_encoder = model.config.encoder\n",
    "config_decoder = model.config.decoder\n",
    "# set decoder config to causal lm\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "\n",
    "# Saving the model, including its configuration\n",
    "model.save_pretrained(\"my-model\")\n",
    "\n",
    "# loading model and config from pretrained folder\n",
    "encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(\"my-model\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"my-model\", config=encoder_decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "generated_ids = model.generate(pixel_values)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
