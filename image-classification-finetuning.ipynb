{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on MIT Indoor Scenes\n",
    "\n",
    "https://www.kaggle.com/itsahmad/indoor-scenes-cvpr-2019\n",
    "\n",
    "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import pprint\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"google/vit-base-patch16-224\"\n",
    "MODEL = \"google/vit-base-patch16-224-in21k\"\n",
    "train_images = \"/Users/vincent/datasets/mit_indoor_scenes/dataset/TrainImages.txt\"\n",
    "test_images = \"/Users/vincent/datasets/mit_indoor_scenes/dataset/TestImages.txt\"\n",
    "train_images = pd.read_table(train_images)\n",
    "test_images = pd.read_table(test_images)\n",
    "data_base_path = \"/Users/vincent/datasets/mit_indoor_scenes/dataset/indoorCVPR_09/Images/\"\n",
    "\n",
    "dataset_train = {}\n",
    "dataset_val = {}\n",
    "dataset_test = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_to_images(df: pd.DataFrame, data_base_path: str):\n",
    "    return [Path(data_base_path, path) for path in df[df.columns[0]]]\n",
    "list_train_and_val = get_path_to_images(train_images, data_base_path)\n",
    "list_test = get_path_to_images(test_images, data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train, list_val = train_test_split(list_train_and_val,train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out JPEG Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jpeg_images(data_base_path: str, dataset: dict) -> dict:\n",
    "    # python dicts are ordered since py3.6\n",
    "    test_files_pixels_map_possible = {}\n",
    "    test_files_pixels_map_not_possible = {}\n",
    "    # for path in Path(data_base_path).rglob(\"*\"):\n",
    "    for path in data_base_path:\n",
    "        path_as_str = str(path)\n",
    "        if os.path.isfile(path):\n",
    "            pixels = PIL.Image.open(path_as_str)\n",
    "            # we only want jpeg type of images to avoid downstream errors\n",
    "            if isinstance(pixels, PIL.JpegImagePlugin.JpegImageFile):\n",
    "                # resize to 224 * 224\n",
    "                pixels.thumbnail((224,224), PIL.Image.ANTIALIAS)\n",
    "                test_files_pixels_map_possible[path_as_str] = pixels\n",
    "            else:\n",
    "                test_files_pixels_map_not_possible[path_as_str] = pixels\n",
    "\n",
    "\n",
    "    pixels = list(test_files_pixels_map_possible.values())\n",
    "    paths = list(test_files_pixels_map_possible.keys())\n",
    "\n",
    "    dataset[\"paths\"] = paths\n",
    "    dataset[\"pixels\"] = pixels\n",
    "    print(\".\", end=\" \")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = get_jpeg_images(data_base_path=list_train, dataset=dataset_train)\n",
    "dataset_val = get_jpeg_images(data_base_path=list_val, dataset=dataset_val)\n",
    "dataset_test = get_jpeg_images(data_base_path=list_test, dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(dataset: dict) -> dict:\n",
    "    \"\"\"Convert pixels into features.\"\"\"\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(MODEL)\n",
    "    print(\".\", end=\" \")\n",
    "    pixel_values = []\n",
    "    dict_ = {\n",
    "        \"paths\": [],\n",
    "        \"pixels\": []\n",
    "        }\n",
    "    for i, image in enumerate(dataset[\"pixels\"]):\n",
    "        try: \n",
    "            batch = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "            # we want to go from \n",
    "            #\n",
    "            #        {'pixel_values': tensor([[[[ 1.0000,  1.0000,  0.5686,  ..., -0.1451, -0.2471, -0.2078],\n",
    "            #                [ 1.0000,  0.9843,  0.5137,  ..., -0.0745,  0.1608,  0.4353],\n",
    "            # to\n",
    "            #\n",
    "            #        {'pixel_values'} : [tensor([[[ 1.0000,  1.0000,  0.5686,  ..., -0.1451, -0.2471, -0.2078],\n",
    "            #                [ 1.0000,  0.9843,  0.5137 ...\n",
    "            # at this point we are not sure why but we are following the notebook from Niels Rogge (link in title)\n",
    "            # pixel_values = [ image for image in batch[\"pixel_values\"]]\n",
    "            pixel_values.append(batch[\"pixel_values\"][0])\n",
    "            # we only keep the records that we can properly process\n",
    "            dict_[\"paths\"].append(dataset[\"paths\"][i])\n",
    "            dict_[\"pixels\"].append(dataset[\"pixels\"][i])\n",
    "        except ValueError as e:\n",
    "            # some pictures cannot be processed. \n",
    "            # we catch the error here, discard the picture \n",
    "            # and continue to the next\n",
    "            print(f\"ValueError on {image} : {e}. discarding ...\")\n",
    "        \n",
    "    dict_[\"pixel_values\"] = pixel_values\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = extract_features(dataset_train)\n",
    "dataset_val = extract_features(dataset_val)\n",
    "dataset_test = extract_features(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_labels(dataset: dict) -> dict:\n",
    "    labels_list = [path.split(\"/\")[-2] for path in dataset[\"paths\"]]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels_list)\n",
    "    print(le.classes_)\n",
    "    return le\n",
    "\n",
    "\n",
    "def transform_labels(dataset: dict, label_encoder:preprocessing.LabelEncoder) -> dict:\n",
    "    labels_list = [path.split(\"/\")[-2] for path in dataset[\"paths\"]]\n",
    "    targets = label_encoder.transform(labels_list)\n",
    "    targets = torch.as_tensor(targets)\n",
    "    dataset[\"labels\"] = targets\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = fit_labels(dataset=dataset_train)\n",
    "targets = [ int(x) for x in list(label_encoder.transform(label_encoder.classes_))]\n",
    "classes = label_encoder.classes_\n",
    "class_no = len(classes)\n",
    "print(f\"classes: {classes}\")\n",
    "print(f\"classes no: {class_no}\")\n",
    "\n",
    "dataset_train = transform_labels(dataset_train, label_encoder)\n",
    "dataset_val = transform_labels(dataset_val, label_encoder)\n",
    "dataset_test = transform_labels(dataset_test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dataset_train[\"labels\"]) == len(dataset_train[\"paths\"]) == len(dataset_train[\"pixel_values\"])\n",
    "assert len(dataset_val[\"labels\"]) == len(dataset_val[\"paths\"]) == len(dataset_val[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_dict(dataset_train)\n",
    "dataset_val = Dataset.from_dict(dataset_val)\n",
    "dataset_test = Dataset.from_dict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # Dataset.from_dict removes the tensor and converts it as a list\n",
    "    # we convert the list here back as a tensor because we lost this\n",
    "    # when converting to a \"Dataset\"\n",
    "    pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "        f\"mit-indoor-scenes\",\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=10,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir='logs',\n",
    "        remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# evaluation\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# model\n",
    "id2label = {id:label for id, label in zip(targets, classes)}\n",
    "label2id = {label:id for id,label in zip(targets, classes )}\n",
    "num_labels = len(classes)\n",
    "\n",
    "\n",
    "pprint.pprint(f\"id2label: {id2label}\")\n",
    "pprint.pprint(f\"label2id: {label2id}\")\n",
    "pprint.pprint(f\"num labels: {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(MODEL)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = list(label2id.keys())\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(100,100))\n",
    "disp.plot(xticks_rotation=45, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push To HUB\n",
    "\n",
    "- the best model was checkpoint 1281.\n",
    "- load this model\n",
    "- create a repo on huggingface to push a model \"vincentclaes/mit-indoor-scenes\"\n",
    "- use the repo path to push the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"mit-indoor-scenes/checkpoint-1281\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"vincentclaes/mit-indoor-scenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "# model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# outputs = model(**inputs)\n",
    "\n",
    "# yeey, we can load the model pushed to the huggingface repo.\n",
    "from transformers import ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"images/coffee-machine-in-office.jpeg\")\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(MODEL)\n",
    "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "model = AutoModel.from_pretrained(\"vincentclaes/mit-indoor-scenes\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_hidden_state': tensor([[[-0.2738,  0.1057,  0.1835,  ..., -0.3797, -0.1796,  0.4518],\n",
       "          [-0.1293, -0.0596, -0.0943,  ..., -0.2971,  0.0511,  0.3609],\n",
       "          [-0.1005, -0.0538, -0.0512,  ..., -0.2894,  0.0361,  0.4253],\n",
       "          ...,\n",
       "          [-0.2806, -0.1837, -0.0868,  ..., -0.0997,  0.0631,  0.4024],\n",
       "          [-0.2779, -0.0071, -0.0388,  ..., -0.0938, -0.1475,  0.4918],\n",
       "          [-0.3208, -0.1019,  0.0198,  ..., -0.2735, -0.1187,  0.5043]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " 'pooler_output': tensor([[ 5.4609e-03,  4.2200e-02, -5.9996e-02,  8.9833e-03,  1.8819e-02,\n",
       "          -3.3277e-02,  2.1682e-01, -2.0431e-02,  1.5139e-02,  1.6508e-01,\n",
       "          -1.5392e-01, -2.0770e-02, -1.3163e-01, -1.0191e-01,  1.0971e-01,\n",
       "           6.0922e-02, -1.5648e-01, -2.3313e-02,  1.7571e-01,  2.2045e-01,\n",
       "           1.1908e-01,  7.4136e-02,  8.9425e-02, -6.4859e-02, -1.2866e-01,\n",
       "          -4.3641e-03,  6.9227e-03, -8.7397e-02,  3.3734e-02,  5.4693e-02,\n",
       "           2.3472e-02, -4.1603e-02,  7.5766e-02,  9.2488e-02, -2.8187e-02,\n",
       "           5.2566e-02, -8.2696e-02,  1.2735e-01, -7.5348e-02, -7.4216e-02,\n",
       "          -3.7692e-02, -7.6854e-02, -7.7225e-02,  7.9277e-02,  1.4350e-02,\n",
       "           2.1585e-01,  3.2236e-02,  3.2705e-02,  1.4741e-01, -8.2258e-02,\n",
       "           1.0895e-02, -3.4833e-02, -1.1355e-01,  1.0478e-01, -1.0257e-01,\n",
       "          -2.0394e-01,  7.1428e-02,  1.9292e-01,  3.6573e-02,  1.5335e-01,\n",
       "           1.1260e-01,  7.8367e-02, -2.1764e-02,  1.1673e-01, -1.3608e-02,\n",
       "           7.5286e-02,  1.7216e-02,  1.1040e-02, -3.5520e-04,  2.1817e-02,\n",
       "           1.1607e-01, -1.4032e-02,  8.7373e-02, -8.1218e-02, -1.4439e-01,\n",
       "          -1.3349e-01, -1.7174e-01,  8.6118e-02, -8.5320e-02, -2.8902e-01,\n",
       "          -1.4560e-02,  1.1563e-01,  6.7034e-02, -1.5189e-01, -8.8468e-02,\n",
       "          -1.2688e-01,  7.4305e-02,  1.8156e-01,  1.6560e-01,  2.3226e-01,\n",
       "           2.5928e-01,  7.2377e-02,  2.8715e-02,  4.8302e-02, -9.9604e-02,\n",
       "           2.0695e-01,  1.7300e-01, -2.3833e-02, -3.0419e-04, -2.1175e-02,\n",
       "          -4.2821e-02, -1.3783e-02,  1.4161e-01,  1.1465e-01,  1.6743e-01,\n",
       "           2.3282e-01, -6.9535e-02, -5.7198e-02,  5.5385e-02, -1.2183e-01,\n",
       "          -2.0621e-03, -1.0976e-01, -9.7178e-02, -6.7887e-02, -5.9353e-02,\n",
       "          -4.0396e-02, -2.9099e-02, -1.3153e-01,  9.1277e-02, -1.4934e-01,\n",
       "           2.1109e-02, -6.1244e-03,  1.5792e-01,  1.4449e-01,  2.0142e-01,\n",
       "           9.3837e-02,  1.2631e-01, -7.4075e-02, -1.0669e-01, -8.7327e-03,\n",
       "           4.3873e-02,  9.8938e-02,  1.1866e-01,  6.3269e-02,  1.9578e-01,\n",
       "           5.9126e-02,  7.5234e-02, -7.3636e-02,  7.4140e-02, -1.0248e-01,\n",
       "           3.4154e-02, -9.6947e-02, -1.8795e-02, -3.1082e-01,  6.4022e-02,\n",
       "          -1.0968e-01, -5.5753e-03,  6.7238e-03,  6.6947e-02,  5.4800e-02,\n",
       "           7.9765e-02,  9.8130e-02, -7.0892e-02, -1.5229e-01,  8.6910e-02,\n",
       "          -2.5352e-01, -8.8455e-02,  5.7226e-02, -1.0963e-01, -2.2055e-01,\n",
       "          -6.8960e-02, -1.7040e-02,  7.5316e-02,  1.7352e-02, -2.6603e-01,\n",
       "           1.9867e-01, -1.1976e-02,  2.1249e-02, -1.3758e-01,  8.4839e-02,\n",
       "          -1.1724e-02,  6.8830e-02, -7.1016e-02,  4.0168e-03,  2.5798e-01,\n",
       "          -8.0319e-02,  4.1521e-02,  5.0006e-02, -1.8687e-01,  1.6836e-02,\n",
       "           9.5515e-02,  1.2104e-02, -9.1418e-02,  8.2260e-02, -1.5257e-01,\n",
       "          -1.6817e-01,  1.0936e-01, -5.2029e-02,  7.4132e-02,  1.2071e-01,\n",
       "          -8.5443e-02, -2.1611e-01,  1.1891e-02,  1.7541e-01,  1.4727e-01,\n",
       "           8.8499e-02, -2.7855e-02, -7.8249e-03,  9.1936e-03,  2.2494e-02,\n",
       "           8.7079e-02, -7.5396e-02, -2.3736e-03, -6.8369e-03, -1.4629e-01,\n",
       "           9.7487e-02,  8.9769e-02,  1.1157e-02, -1.2800e-01,  2.5065e-02,\n",
       "           2.4978e-02,  1.1716e-01,  2.0502e-01,  4.0166e-02, -2.4605e-01,\n",
       "           2.2377e-01,  5.2575e-02, -6.7787e-02, -5.6154e-02,  2.7705e-01,\n",
       "           6.6051e-02,  1.1347e-01,  5.9893e-02,  1.5759e-01,  1.7547e-02,\n",
       "          -4.5014e-02,  2.0909e-01, -6.6274e-02, -7.9870e-03, -4.7635e-02,\n",
       "          -2.1564e-01, -1.8632e-01, -1.7236e-02, -6.5188e-02, -5.0603e-02,\n",
       "           2.8280e-02,  1.3076e-01,  1.3247e-01, -3.4641e-02, -2.7213e-01,\n",
       "           3.4428e-02, -6.0063e-02, -1.0852e-01,  1.5057e-01,  8.6422e-02,\n",
       "           1.3123e-01,  2.0696e-01, -7.5439e-02,  3.1719e-01, -1.6403e-01,\n",
       "           7.6797e-02, -2.0083e-01,  4.4559e-02, -1.1647e-01,  2.4900e-01,\n",
       "          -1.7085e-01, -1.5515e-02,  2.2290e-02, -4.1795e-02,  5.8739e-02,\n",
       "           1.4703e-01,  1.5490e-01, -9.0884e-02,  2.6631e-01,  8.3329e-02,\n",
       "           3.7380e-02,  6.4305e-02,  2.5809e-02,  1.8267e-02, -9.0856e-02,\n",
       "           3.2212e-02, -6.0061e-02, -1.6403e-01,  4.7886e-02, -4.3150e-02,\n",
       "           2.3891e-01,  8.0411e-02,  1.4673e-01, -1.0795e-01,  6.8495e-02,\n",
       "           9.4269e-02,  1.6192e-01,  5.2505e-02, -2.1193e-03,  9.0145e-02,\n",
       "          -3.8890e-02,  3.7449e-02,  1.6010e-02,  4.2076e-02,  1.7019e-01,\n",
       "           1.0938e-01,  5.6333e-02,  1.7436e-01, -1.9938e-02,  4.0725e-03,\n",
       "          -1.5564e-01, -7.9673e-02, -1.2653e-01,  8.1355e-02,  9.5571e-02,\n",
       "          -1.5364e-01,  1.2224e-01,  4.8403e-02,  6.2305e-02,  1.3612e-01,\n",
       "           3.0106e-02,  3.8957e-02, -1.3678e-02, -8.8206e-02, -7.9630e-02,\n",
       "          -5.4455e-02, -3.9433e-03,  1.0179e-01,  5.8491e-03, -5.6204e-02,\n",
       "          -8.7707e-02,  8.8400e-02, -5.6545e-02,  1.3098e-01,  1.8991e-01,\n",
       "          -1.4780e-01,  5.3508e-02, -1.3127e-02, -3.3409e-02, -3.0389e-02,\n",
       "          -1.1736e-01,  1.1548e-01,  1.8591e-01,  1.2040e-01, -3.1007e-02,\n",
       "          -1.0119e-01,  5.4124e-03,  2.9887e-01, -2.9257e-01,  4.6793e-03,\n",
       "          -4.3574e-02, -1.6516e-01,  1.0448e-01,  1.2269e-02, -2.0755e-01,\n",
       "           1.0874e-01,  6.4180e-02, -2.0622e-01, -2.7907e-02,  2.0933e-01,\n",
       "          -2.3693e-01,  2.8061e-02,  1.7228e-01, -2.4170e-01, -9.1755e-03,\n",
       "           5.6934e-02,  1.9797e-01,  1.0983e-01, -8.2965e-02,  5.1255e-03,\n",
       "          -1.6548e-02, -6.1012e-02,  2.5092e-02, -1.8100e-01,  2.5666e-02,\n",
       "          -9.6318e-03, -1.9363e-02,  6.2824e-02,  5.5621e-02, -4.1112e-02,\n",
       "          -1.4141e-01,  2.2403e-02,  1.4990e-01,  1.4862e-02, -1.4279e-01,\n",
       "           4.1004e-02, -1.1277e-01, -2.2139e-01, -6.6679e-02,  1.3351e-01,\n",
       "          -3.1990e-02, -8.5875e-02, -1.3587e-02,  6.4009e-02,  3.1457e-03,\n",
       "           8.8393e-02,  5.7978e-03,  1.0520e-02, -1.0691e-01,  9.6020e-02,\n",
       "           5.6197e-02,  4.6110e-02, -4.0862e-02, -2.7868e-03, -1.5676e-01,\n",
       "          -3.2363e-03, -1.3630e-01, -2.3759e-02,  1.0255e-01,  1.0071e-01,\n",
       "          -1.1152e-01, -3.4901e-02,  5.2396e-02, -2.0126e-01, -9.1700e-02,\n",
       "           1.9261e-01, -7.7931e-03, -1.8963e-01, -2.9008e-02, -1.2955e-01,\n",
       "           7.3345e-02,  7.0949e-02,  7.3251e-02, -9.6170e-02, -9.3969e-02,\n",
       "          -8.0267e-02, -9.9009e-02, -1.5911e-02,  2.3647e-02,  1.2689e-01,\n",
       "           1.3995e-01,  1.0776e-01, -5.5891e-02, -6.0928e-02, -1.4219e-02,\n",
       "          -9.8966e-02,  8.3762e-02,  8.7091e-02, -1.5314e-01,  1.6245e-01,\n",
       "          -1.5754e-01, -1.3804e-01,  2.9948e-01, -1.6510e-01, -8.8579e-02,\n",
       "           1.4553e-01,  2.4492e-01, -4.7449e-03, -1.5469e-01, -1.7331e-01,\n",
       "           6.5229e-02,  1.2912e-02, -1.6146e-01, -2.1664e-01, -3.5060e-03,\n",
       "           1.2215e-01, -2.1787e-01, -1.9345e-02, -5.2615e-02, -6.9638e-02,\n",
       "           2.1549e-01, -1.5211e-01, -4.8780e-02,  2.6966e-02, -1.6297e-02,\n",
       "          -1.0002e-01,  4.4661e-03, -8.4086e-02,  3.9147e-03,  8.2273e-02,\n",
       "           5.3960e-02, -1.5591e-01,  8.7094e-02,  6.6232e-02, -8.3345e-02,\n",
       "           1.7909e-01,  1.1342e-01,  1.1364e-01,  1.4062e-01, -4.6041e-02,\n",
       "           3.8724e-02, -1.1866e-01,  1.0443e-01,  1.0023e-02,  7.3302e-02,\n",
       "           2.3325e-01, -1.7193e-02, -1.6804e-02, -3.7779e-01,  6.8414e-02,\n",
       "          -4.0976e-02,  3.1259e-02, -2.7330e-02, -6.0016e-02, -4.3107e-02,\n",
       "          -6.0108e-02, -1.2214e-01, -1.0805e-02, -5.1324e-02,  3.4249e-02,\n",
       "           9.2874e-02, -1.4149e-01,  1.0466e-01,  1.3546e-01,  1.2033e-01,\n",
       "           1.8592e-02,  1.5368e-01, -6.4280e-02, -1.6841e-01, -1.1994e-01,\n",
       "           8.8669e-02, -6.5291e-02, -9.2864e-02, -3.8797e-02,  7.1597e-02,\n",
       "          -4.8154e-03, -5.5370e-02, -2.4576e-02, -4.6929e-02, -3.5133e-02,\n",
       "          -5.1873e-02, -6.5571e-02,  1.2939e-01,  1.3337e-01, -1.0642e-01,\n",
       "           2.6738e-01,  1.0096e-01, -1.4963e-01, -1.4505e-02,  9.5023e-02,\n",
       "          -1.3498e-02, -1.9821e-01,  6.0269e-02,  3.4142e-02, -1.1731e-01,\n",
       "           4.0117e-02,  7.1671e-02, -1.4597e-01, -9.3089e-02,  1.1641e-02,\n",
       "           1.6467e-01,  2.0243e-02,  3.0168e-02,  7.1125e-02,  4.9855e-02,\n",
       "          -7.9356e-02,  1.3476e-01, -9.0849e-02, -2.7962e-02, -3.8988e-02,\n",
       "           8.6525e-02, -1.7123e-01,  2.2169e-01, -1.0169e-01, -1.7966e-02,\n",
       "           3.4508e-02,  4.2612e-02, -2.5629e-01,  2.3575e-02,  1.4129e-01,\n",
       "          -1.0363e-01,  3.1683e-01, -5.0941e-02,  7.2138e-02,  8.2439e-02,\n",
       "          -2.3643e-02,  1.8236e-01, -1.2344e-01,  2.6944e-01, -1.2699e-01,\n",
       "           2.2770e-02,  1.2149e-01, -6.3407e-02,  2.9649e-02, -5.2760e-02,\n",
       "           1.1417e-01,  2.3432e-01, -2.6535e-02,  5.7310e-02,  1.7955e-01,\n",
       "          -1.3220e-01, -1.6869e-01, -1.0584e-01, -9.3327e-02, -2.4389e-01,\n",
       "           9.9031e-02,  3.5583e-02, -2.2788e-02,  2.6092e-02,  1.5262e-01,\n",
       "           6.3922e-02,  1.4292e-01,  1.3650e-01,  7.1886e-02,  1.0597e-02,\n",
       "           2.1702e-02,  4.2362e-02,  1.9405e-01,  1.7829e-01,  1.9214e-02,\n",
       "          -1.4840e-02,  2.3043e-01,  1.7112e-01,  3.9686e-02, -2.5571e-02,\n",
       "           1.7656e-02,  6.5583e-02,  1.3734e-02, -1.1365e-01, -4.0823e-02,\n",
       "           1.4520e-02,  7.9723e-02,  8.0183e-03, -1.1230e-01,  1.0985e-02,\n",
       "           4.6267e-02, -8.8101e-02, -1.0579e-01,  7.8526e-02,  1.3375e-01,\n",
       "           3.1253e-02,  3.3200e-02, -1.4659e-01,  2.2688e-01,  1.8472e-01,\n",
       "          -4.8154e-02,  1.0728e-01, -1.7594e-02, -6.7148e-02, -1.1147e-01,\n",
       "           9.9745e-02,  4.8383e-02, -1.2216e-01, -1.2366e-01, -8.2131e-02,\n",
       "           4.1044e-02,  8.0174e-02,  1.5165e-01,  1.7647e-01,  9.9870e-03,\n",
       "          -2.2528e-01,  5.9966e-02, -9.3522e-03, -7.9375e-02, -5.3995e-02,\n",
       "          -1.3692e-01, -5.7809e-02, -2.1164e-02, -6.8646e-02, -5.6615e-03,\n",
       "          -4.5112e-02, -2.1596e-01, -9.6437e-02,  5.3476e-02,  6.5347e-02,\n",
       "           1.3207e-02,  3.2137e-03,  1.4353e-01, -2.6615e-02,  1.6174e-01,\n",
       "           1.0977e-01, -2.0052e-01, -2.4885e-02,  1.0676e-01,  7.2787e-02,\n",
       "           9.5048e-02,  1.5219e-01, -1.6189e-01,  1.0182e-01, -5.7652e-02,\n",
       "          -1.5102e-01, -9.5357e-02, -7.7011e-02, -1.9538e-01,  6.0346e-02,\n",
       "          -1.3241e-02,  2.6627e-03,  1.3429e-01, -2.7751e-02, -1.5168e-01,\n",
       "          -2.2298e-02,  1.4064e-01, -1.2115e-01,  7.9953e-02,  4.3185e-02,\n",
       "          -2.0262e-03,  1.6903e-01, -1.4133e-01,  9.7479e-02,  3.9021e-02,\n",
       "          -4.0205e-02, -6.3401e-02, -8.7903e-02, -1.3915e-01,  5.9058e-02,\n",
       "          -2.4677e-02, -3.7260e-02, -1.0420e-01, -6.5802e-02,  2.7938e-02,\n",
       "           7.8341e-02, -1.5943e-01,  2.2833e-01, -1.9691e-02,  1.6846e-01,\n",
       "           7.5786e-02, -3.6653e-03,  8.9592e-02, -9.7020e-02, -1.6722e-01,\n",
       "           1.8786e-01,  2.8908e-01, -5.4815e-03,  2.5941e-02, -7.9351e-02,\n",
       "          -7.0755e-02,  3.2995e-02,  7.8515e-02, -2.4560e-02, -2.3136e-02,\n",
       "           4.0673e-02, -1.6290e-01, -7.8755e-02,  4.2532e-02, -8.7447e-03,\n",
       "           5.6165e-02, -1.7149e-01,  2.1204e-02, -7.7510e-02,  2.4538e-01,\n",
       "          -4.3481e-02, -1.9749e-01, -1.0403e-01, -1.8249e-01,  6.9264e-02,\n",
       "          -1.5702e-01,  1.2484e-01, -1.1830e-01, -2.6265e-01,  2.0292e-02,\n",
       "           1.2771e-01, -3.5509e-02,  1.4348e-01, -6.6462e-02, -6.2979e-02,\n",
       "           1.0557e-01, -3.8500e-03, -7.3714e-02, -2.6365e-01,  7.1382e-02,\n",
       "           1.3031e-01, -2.6671e-01, -2.4195e-01,  5.6977e-03, -5.4410e-02,\n",
       "           1.1940e-01, -1.3414e-01, -2.7979e-02, -7.2738e-02,  2.6964e-02,\n",
       "          -6.6249e-02,  4.3513e-02, -4.4860e-02, -2.9519e-02, -1.0331e-01,\n",
       "           1.0935e-01,  2.7636e-04,  4.5528e-02,  8.9529e-02, -1.1613e-01,\n",
       "          -5.6458e-02, -3.0065e-01, -3.7189e-02, -1.7514e-01, -1.2423e-01,\n",
       "           1.0289e-01, -5.9966e-02,  1.2722e-02, -1.7413e-02,  5.6300e-02,\n",
       "           4.3321e-02, -2.4491e-01,  6.6276e-02]], grad_fn=<TanhBackward0>),\n",
       " 'hidden_states': None,\n",
       " 'attentions': None}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vars(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
